<<<<<<< Updated upstream
fmp_acc[i] <- s2[1]
fmp_time[i] <- fmp_end_time - fmp_start_time
vp_acc[i] <- s3[1]
vp_time[i] <- vp_end_time - vp_start_time
print(sprintf("perc: %s | com perc: %s Z voted perc: %s \n\n", s1[1], s2[1], s3[1]))
}
return(list(data.frame("perceptron" = perc_acc, "com_perceptron" = fmp_acc, "voted_perceptron" = vp_acc),
data.frame("perceptron" = perc_time, "com_perceptron" = fmp_time, "voted_perceptron" = vp_time)))
=======
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (c(cm$overall[1], err[length(err)]))
}
### FUNÇÕES PARA TESTAR O MÉTODO ###
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
# separação em treinamento e teste
suffled_indexes <- sample(nt)
train_size <- nt * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(train_size+1):nt], cbind(1,2)]
y_test <- X[suffled_indexes[(train_size+1):nt], 3]
# treinamento do perceptron
sol <- train_voted_perceptron(X_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
c <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(X_test)[1], ncol=1)
for (i in 1:dim(X_test)[1]){
s <- y_voted_percetron(X_test[i,], w, c,-1)
ypred[i] <- s
}
correct <- length(ypred[ypred==y_test])
total <- length(ypred)
acc <- correct/total
print(c("acc:", acc))
# matriz de confusão
lvs <- c("1", "0")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
# load Caret package for computing Confusion matrix
library(caret)
confusionMatrix(xtab)
seqi<-seq(0,6, 0.1)
seqj<-seq(0,6, 0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-y_voted_percetron(x, w, c, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
#perceptron
yperceptron <- function(xvec, w, pol){
# xvec: vetor de entrada
# w: vetor de pesos
# yp: resposta do Perceptron
u<- xvec %*% w[2:length(w)] + pol * w[1]
y<-1.0*(u>=0)
return(as.matrix(y))
}
# aplicação do perceptron para separação
trainperceptron <- function(xin, yd, eta, tol, maxepocas, par){
# xin: matrin N x n com dados de entrada
# yd: rótulos de saída
# eta: passo do treinamento
# tol: tolerancia de erro
# maxepocas: numero maximo de iterações
# par: parametro de entrada
# par=0 ==> xin tem dimensão n+1 e já inclui entrada correspondente ao temo de polarização
# par =1 ==> xin tem dimensão n e não inclui entrada corresponde ao termo de polarização, que deve ser adicionado dentro da função
dimxin<-dim(xin)
N<-dimxin[1]
n<-dimxin[2]
if(par==1){
wt<-as.matrix(runif(n+1) - 0.5)
xin<-cbind(-1,xin)
}
else{
wt<-as.matrix(runif(n)-0.5)
}
nepocas<-0
eepoca<-tol+1
evec<-matrix(nrow=1,ncol=maxepocas)
while((nepocas<maxepocas && eepoca>tol)){
ei2<-0
xseq<-sample(N)
for(i in 1:N){
irand<-xseq[i]
yhati<-1.0*((xin[irand,] %*% wt)>=0)
ei<-yd[irand]-yhati
dw<-eta*ei*xin[irand,]
wt<-wt+dw
ei2<-ei2+ei*ei
}
nepocas<-nepocas+1
evec[nepocas]<-ei2/N
eepoca<-evec[nepocas]
}
retlist<-list(wt, evec[1:nepocas])
return(retlist)
}
eval_perceptron <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do perceptron
sol <- trainperceptron(x_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- yperceptron(x_test[i,], w, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("0", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
>>>>>>> Stashed changes
}
evaluate_dataset <- function(X, Y, name, num_exec){
results <- multiple_evaluations(X, Y, num_exec)
accs <- results[[1]]
times <- results[[2]]
par(mfrow=c(2,2))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(accs, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(times, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Tempo (%s)",name))
# Aplicação do kruskal e teste de nemenyi
accs_matrix <- as.matrix(accs)
tsutils::nemenyi(accs_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Acurácias)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
# Aplicação do kruskal e teste de nemenyi
time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
<<<<<<< Updated upstream
######################################### TWO GAUSSIAN DATASET #########################################
nc = 500
xc1 <- matrix(0.35 * rnorm(nc) + 2, ncol = 2)
xc2 <- matrix(0.35 * rnorm(nc) + 4, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
evaluate_dataset(X[,1:2], X[,3], name, 50)
rm(list=ls())
graphics.off()
source('perceptron.R')
source('votedPerceptron.R')
source('commiteePerceptron.R')
library(caret)
# MAIN
multiple_evaluations <- function(X, Y, num_eval){
perc_acc <- matrix(0, nrow=num_eval, ncol=1)
perc_time <- matrix(0, nrow=num_eval, ncol=1)
fmp_acc <- matrix(0, nrow=num_eval, ncol=1)
fmp_time <- matrix(0, nrow=num_eval, ncol=1)
vp_acc <- matrix(0, nrow=num_eval, ncol=1)
vp_time <- matrix(0, nrow=num_eval, ncol=1)
for (i in 1:num_eval){
print(sprintf("Time for perceptron - %s", i))
perc_start_time <- Sys.time()
s1 <- eval_perceptron(X, Y, should_plot_matrix=FALSE)
perc_end_time <- Sys.time()
print(sprintf("Time for commitee perceptron - %s", i))
fmp_start_time <- Sys.time()
s2 <- eval_com_perceptron(X, Y, should_plot_matrix=FALSE)
fmp_end_time <- Sys.time()
print(sprintf("Time for voted perceptron - %s", i))
vp_start_time <- Sys.time()
s3 <- eval_voted_perceptron(X, Y, should_plot_matrix=FALSE)
vp_end_time <- Sys.time()
perc_acc[i] <- s1[1]
perc_time[i] <- perc_end_time - perc_start_time
fmp_acc[i] <- s2[1]
fmp_time[i] <- fmp_end_time - fmp_start_time
vp_acc[i] <- s3[1]
vp_time[i] <- vp_end_time - vp_start_time
print(sprintf("perc: %s | com perc: %s Z voted perc: %s \n\n", s1[1], s2[1], s3[1]))
}
return(list(data.frame("perceptron" = perc_acc, "com_perceptron" = fmp_acc, "voted_perceptron" = vp_acc),
data.frame("perceptron" = perc_time, "com_perceptron" = fmp_time, "voted_perceptron" = vp_time)))
}
evaluate_dataset <- function(X, Y, name, num_exec){
results <- multiple_evaluations(X, Y, num_exec)
accs <- results[[1]]
times <- results[[2]]
par(mfrow=c(2,2))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(accs, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(times, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Tempo (%s)",name))
# Aplicação do kruskal e teste de nemenyi
accs_matrix <- as.matrix(accs)
tsutils::nemenyi(accs_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Acurácias)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
# Aplicação do kruskal e teste de nemenyi
time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
######################################### TWO GAUSSIAN DATASET #########################################
nc = 500
xc1 <- matrix(0.35 * rnorm(nc) + 2, ncol = 2)
xc2 <- matrix(0.35 * rnorm(nc) + 4, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
evaluate_dataset(X[,1:2], X[,3], name, 50)
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 1 com classe 2
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
# plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 5]
evaluate_dataset(X, Y, name, 50)
name <- "Wine Dataset"
wine.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read.csv(wine.url, header=FALSE)
# pre-processamento
wine <- wine[1:130,]# filtra apenas dois tipos de vinho, para que fique binario
wine$V1[wine$V1 == 2] <- -1
wine$V1 <- as.factor(wine$V1)
names(wine)[1] <- 'y'
# converter em matrix
mat <- data.matrix(wine)
X <- scale(mat[, 2:14], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 1]
evaluate_dataset(X, Y, name, 50)
rm(list=ls())
graphics.off()
source('perceptron.R')
source('votedPerceptron.R')
source('commiteePerceptron.R')
library(caret)
# MAIN
multiple_evaluations <- function(X, Y, num_eval){
perc_acc <- matrix(0, nrow=num_eval, ncol=1)
perc_time <- matrix(0, nrow=num_eval, ncol=1)
fmp_acc <- matrix(0, nrow=num_eval, ncol=1)
fmp_time <- matrix(0, nrow=num_eval, ncol=1)
vp_acc <- matrix(0, nrow=num_eval, ncol=1)
vp_time <- matrix(0, nrow=num_eval, ncol=1)
for (i in 1:num_eval){
print(sprintf("Iteration - %s", i))
perc_start_time <- Sys.time()
s1 <- eval_perceptron(X, Y, should_plot_matrix=FALSE)
perc_end_time <- Sys.time()
fmp_start_time <- Sys.time()
s2 <- eval_com_perceptron(X, Y, should_plot_matrix=FALSE)
fmp_end_time <- Sys.time()
vp_start_time <- Sys.time()
s3 <- eval_voted_perceptron(X, Y, should_plot_matrix=FALSE)
vp_end_time <- Sys.time()
perc_acc[i] <- s1[1]
perc_time[i] <- perc_end_time - perc_start_time
fmp_acc[i] <- s2[1]
fmp_time[i] <- fmp_end_time - fmp_start_time
vp_acc[i] <- s3[1]
vp_time[i] <- vp_end_time - vp_start_time
print(sprintf("perc: %s | com perc: %s | voted perc: %s \n\n", s1[1], s2[1], s3[1]))
}
return(list(data.frame("perceptron" = perc_acc, "perceptron_commitee" = fmp_acc, "voted_perceptron" = vp_acc),
data.frame("perceptron" = perc_time, "perceptron_commitee" = fmp_time, "voted_perceptron" = vp_time)))
}
evaluate_dataset <- function(X, Y, name, num_exec){
results <- multiple_evaluations(X, Y, num_exec)
accs <- results[[1]]
times <- results[[2]]
par(mfrow=c(2,2))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(accs, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(times, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Tempo (%s)",name))
# Aplicação do kruskal e teste de nemenyi
accs_matrix <- as.matrix(accs)
tsutils::nemenyi(accs_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Acurácias)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
# Aplicação do kruskal e teste de nemenyi
time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
######################################### TWO GAUSSIAN DATASET #########################################
nc = 500
xc1 <- matrix(0.35 * rnorm(nc) + 2, ncol = 2)
xc2 <- matrix(0.35 * rnorm(nc) + 4, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
evaluate_dataset(X[,1:2], X[,3], name, 50)
name <- "Wine Dataset"
wine.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read.csv(wine.url, header=FALSE)
# pre-processamento
wine <- wine[1:130,]# filtra apenas dois tipos de vinho, para que fique binario
wine$V1[wine$V1 == 2] <- -1
wine$V1 <- as.factor(wine$V1)
names(wine)[1] <- 'y'
# converter em matrix
mat <- data.matrix(wine)
X <- scale(mat[, 2:14], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 1]
evaluate_dataset(X, Y, name, 50)
name <- "Pima Indians"
pima <- data(PimaIndiansDiabetes)
names(PimaIndiansDiabetes)[9] <- "y"
# converter em matrix
mat <- data.matrix(PimaIndiansDiabetes)
X <- scale(mat[, 1:8], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 9]
evaluate_dataset(X, Y, name, 100)
# load the dataset
name <- "Pima Indians"
pima <- data(PimaIndiansDiabetes)
names(PimaIndiansDiabetes)[9] <- "y"
# converter em matrix
mat <- data.matrix(PimaIndiansDiabetes)
X <- scale(mat[, 1:8], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 9]
Y
X
mat
mat <- data.matrix(PimaIndiansDiabetes)
Y <- mat[, 1]
X <- scale(mat[, 2:8], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y
y - 1
Y - 1
# converter em matrix
mat <- data.matrix(wine)
X <- scale(mat[, 2:14], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 1]
Y
name <- "Wine Dataset"
wine.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read.csv(wine.url, header=FALSE)
# pre-processamento
wine <- wine[1:130,]# filtra apenas dois tipos de vinho, para que fique binario
wine$V1[wine$V1 == 2] <- -1
wine$V1 <- as.factor(wine$V1)
names(wine)[1] <- 'y'
# converter em matrix
mat <- data.matrix(wine)
X <- scale(mat[, 2:14], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 1] - 1 # transformando 3
evaluate_dataset(X, Y, name, 50)
# load the dataset
name <- "Pima Indians"
pima <- data(PimaIndiansDiabetes)
names(PimaIndiansDiabetes)[9] <- "y"
# converter em matrix
mat <- data.matrix(PimaIndiansDiabetes)
Y <- mat[, 1] - 1 # transformando entradas em 0 e 1
X <- scale(mat[, 2:8], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
evaluate_dataset(X, Y, name, 100)
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 1 com classe 2
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
# plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 5]
evaluate_dataset(X, Y, name, 50)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 5]
Y
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 1 com classe 2
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
# plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 5] - 1 # transformando em 0 e 1
evaluate_dataset(X, Y, name, 50)
mat <- data.matrix(cae)
X <- scale(mat[, 1:5], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 6]
name <- "Caesarian Dataset"
cae <- read.csv('caesarian.csv', header=TRUE)
# pre processamento
cae$y <- as.factor(cae$y)
# converter em matrix
mat <- data.matrix(cae)
X <- scale(mat[, 1:5], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 6]
name <- "Breast Cancer Dataset"
bc.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
bc <- read.csv(bc.url, header=FALSE)
bc <- bc[,2:11]
bc <- na.omit(bc)
bc <- droplevels(bc[!bc$V7 == '?',])
# pre processamento
bc$V11 <- as.factor(bc$V11)
names(bc)[10] <- "y"
# converter em matrix
mat <- data.matrix(bc)
Y <- mat[, 10]
Y
#########################################  CERVICAL CANCER DATASET #########################################
name <- "Breast Cancer Dataset"
bc.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
bc <- read.csv(bc.url, header=FALSE)
bc <- bc[,2:11]
bc <- na.omit(bc)
bc <- droplevels(bc[!bc$V7 == '?',])
# pre processamento
bc$V11 <- as.factor(bc$V11)
names(bc)[10] <- "y"
# converter em matrix
mat <- data.matrix(bc)
Y <- mat[, 10] - 1 # subtraindo para ficar entre 0 e 1
X <- scale(mat[, 1:9], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
evaluate_dataset(X, Y, name, 100)
rm(list=ls())
graphics.off()
source('perceptron.R')
source('votedPerceptron.R')
source('commiteePerceptron.R')
library(caret)
# MAIN
multiple_evaluations <- function(X, Y, num_eval){
perc_acc <- matrix(0, nrow=num_eval, ncol=1)
perc_time <- matrix(0, nrow=num_eval, ncol=1)
fmp_acc <- matrix(0, nrow=num_eval, ncol=1)
fmp_time <- matrix(0, nrow=num_eval, ncol=1)
vp_acc <- matrix(0, nrow=num_eval, ncol=1)
vp_time <- matrix(0, nrow=num_eval, ncol=1)
for (i in 1:num_eval){
print(sprintf("Iteration - %s", i))
perc_start_time <- Sys.time()
s1 <- eval_perceptron(X, Y, should_plot_matrix=FALSE)
perc_end_time <- Sys.time()
fmp_start_time <- Sys.time()
s2 <- eval_com_perceptron(X, Y, should_plot_matrix=FALSE)
fmp_end_time <- Sys.time()
vp_start_time <- Sys.time()
s3 <- eval_voted_perceptron(X, Y, should_plot_matrix=FALSE)
vp_end_time <- Sys.time()
perc_acc[i] <- s1[1]
perc_time[i] <- perc_end_time - perc_start_time
fmp_acc[i] <- s2[1]
fmp_time[i] <- fmp_end_time - fmp_start_time
vp_acc[i] <- s3[1]
vp_time[i] <- vp_end_time - vp_start_time
print(sprintf("perc: %s | com perc: %s | voted perc: %s \n\n", s1[1], s2[1], s3[1]))
}
return(list(data.frame("perceptron" = perc_acc, "perceptron_commitee" = fmp_acc, "voted_perceptron" = vp_acc),
data.frame("perceptron" = perc_time, "perceptron_commitee" = fmp_time, "voted_perceptron" = vp_time)))
}
evaluate_dataset <- function(X, Y, name, num_exec){
results <- multiple_evaluations(X, Y, num_exec)
accs <- results[[1]]
times <- results[[2]]
par(mfrow=c(2,2))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(accs, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(times, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Tempo (%s)",name))
# Aplicação do kruskal e teste de nemenyi
accs_matrix <- as.matrix(accs)
tsutils::nemenyi(accs_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Acurácias)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
# Aplicação do kruskal e teste de nemenyi
time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
######################################### TWO GAUSSIAN DATASET #########################################
nc = 500
xc1 <- matrix(0.35 * rnorm(nc) + 2, ncol = 2)
xc2 <- matrix(0.35 * rnorm(nc) + 4, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
evaluate_dataset(X[,1:2], X[,3], name, 50)
######################################### IRIS DATASET #########################################
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 1 com classe 2
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
# plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 5] - 1 # transformando em 0 e 1
evaluate_dataset(X, Y, name, 50)
######################################### WINE DATASET #########################################
name <- "Wine Dataset"
wine.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read.csv(wine.url, header=FALSE)
# pre-processamento
wine <- wine[1:130,]# filtra apenas dois tipos de vinho, para que fique binario
wine$V1[wine$V1 == 2] <- -1
wine$V1 <- as.factor(wine$V1)
names(wine)[1] <- 'y'
# converter em matrix
mat <- data.matrix(wine)
X <- scale(mat[, 2:14], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 1] - 1 # transformando em 0 e 1
evaluate_dataset(X, Y, name, 50)
######################################### PimaIndiansDiabetes #########################################
# load the dataset
name <- "Pima Indians"
pima <- data(PimaIndiansDiabetes)
names(PimaIndiansDiabetes)[9] <- "y"
# converter em matrix
mat <- data.matrix(PimaIndiansDiabetes)
Y <- mat[, 1] - 1 # transformando entradas em 0 e 1
X <- scale(mat[, 2:8], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
evaluate_dataset(X, Y, name, 50)
#########################################  CERVICAL CANCER DATASET #########################################
name <- "Breast Cancer Dataset"
bc.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
bc <- read.csv(bc.url, header=FALSE)
bc <- bc[,2:11]
bc <- na.omit(bc)
bc <- droplevels(bc[!bc$V7 == '?',])
# pre processamento
bc$V11 <- as.factor(bc$V11)
names(bc)[10] <- "y"
# converter em matrix
mat <- data.matrix(bc)
Y <- mat[, 10] - 1 # subtraindo para ficar entre 0 e 1
X <- scale(mat[, 1:9], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
evaluate_dataset(X, Y, name, 50)
#########################################  CERVICAL CANCER DATASET #########################################
name <- "Cervical Cancer Dataset"
bc.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
bc <- read.csv(bc.url, header=FALSE)
bc <- bc[,2:11]
bc <- na.omit(bc)
bc <- droplevels(bc[!bc$V7 == '?',])
# pre processamento
bc$V11 <- as.factor(bc$V11)
names(bc)[10] <- "y"
# converter em matrix
mat <- data.matrix(bc)
Y <- mat[, 10] - 1 # subtraindo para ficar entre 0 e 1
X <- scale(mat[, 1:9], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
evaluate_dataset(X, Y, name, 50)
=======
# gerar mais amostras da mesma distribuição
# criação de dataset com duas distribuições normais
s1 <- 0.4
s2 <- 0.4
nc <- 200
nt <- nc * 2
### FUNÇÕES PARA TESTAR O MÉTODO ###
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
# separação em treinamento e teste
suffled_indexes <- sample(nt)
train_size <- nt * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(train_size+1):nt], cbind(1,2)]
y_test <- X[suffled_indexes[(train_size+1):nt], 3]
# treinamento do perceptron
sol <- trainperceptron(X_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(X_test)[1], ncol=1)
for (i in 1:dim(X_test)[1]){
s <- yperceptron(X_test[i,], w, -1)
ypred[i] <- s
}
correct <- length(ypred[ypred==y_test])
total <- length(ypred)
acc <- correct/total
print(c("acc:", acc))
# matriz de confusão
lvs <- c("1", "0")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
# load Caret package for computing Confusion matrix
library(caret)
confusionMatrix(xtab)
seqi<-seq(0,6, 0.1)
seqj<-seq(0,6, 0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-yperceptron(x, w, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
#perceptron
y_voted_percetron <- function(xvec, w, c, pol){
# xvec: vetor de entrada
# w: vetor de pesos
# yp: resposta do Perceptron
s <- 0
for (i in 1:nrow(w)){
u <- xvec %*% w[i, 2:ncol(w)] + pol * w[i, 1]
s <- s + c[i] * sign(u)
}
return(sign(s))
}
# aplicação do perceptron para separação
train_voted_perceptron <- function(xin, yd, eta, tol, maxepocas, par){
# xin: matrin N x n com dados de entrada
# yd: rótulos de saída
# eta: passo do treinamento
# tol: tolerancia de erro
# maxepocas: numero maximo de iterações
# par: parametro de entrada
# par=0 ==> xin tem dimensão n+1 e já inclui entrada correspondente ao temo de polarização
# par =1 ==> xin tem dimensão n e não inclui entrada corresponde ao termo de polarização, que deve ser adicionado dentro da função
dimxin<-dim(xin)
N<-dimxin[1]
n<-dimxin[2]
if(par==1){
wt<-as.matrix(runif(n+1) - 0.5)
xin<-cbind(-1,xin)
}
else{
wt<-as.matrix(runif(n)-0.5)
}
nepocas<-0
eepoca<-tol+1
evec<-matrix(nrow=1,ncol=maxepocas)
wt <- matrix(0, ncol=ncol(xin), nrow=N)
c <- matrix(0, ncol=1, nrow=N)
while((nepocas<maxepocas && eepoca>tol)){
ei2<-0
xseq<-sample(N)
k <- 1
for(i in 1:N){
irand<-xseq[i]
yhati<-1.0*((xin[irand,] %*% wt[k,])>=0)
ei<-yd[irand]-yhati
if (ei == 0){ # acertou
c[k] <- c[k] + 1
} else { # errou
dw<-eta*ei*xin[irand,]
wt[k + 1,] <- wt[k,] + dw
c[k + 1,] <- 1
ei2<-ei2+ei*ei
k <- k + 1
}
}
nepocas<-nepocas+1
evec[nepocas]<-ei2/N
eepoca<-evec[nepocas]
}
retlist<-list(wt[1:k,], c[1:k,], evec[1:nepocas])
return(retlist)
}
eval_voted_perceptron <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do voted perceptron
sol <- train_voted_perceptron(x_train, y_train, 0.01, 0.01, 50, 1)
v <- sol[[1]]
c <- sol[[2]]
err <- c(0)
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- y_voted_percetron(x_test[i,], v, c, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("-1", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (c(cm$overall[1], err[length(err)]))
}
### FUNÇÕES PARA TESTAR O MÉTODO ###
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
# separação em treinamento e teste
suffled_indexes <- sample(nt)
train_size <- nt * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(train_size+1):nt], cbind(1,2)]
y_test <- X[suffled_indexes[(train_size+1):nt], 3]
# treinamento do perceptron
sol <- train_voted_perceptron(X_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
c <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(X_test)[1], ncol=1)
for (i in 1:dim(X_test)[1]){
s <- y_voted_percetron(X_test[i,], w, c,-1)
ypred[i] <- s
}
correct <- length(ypred[ypred==y_test])
total <- length(ypred)
acc <- correct/total
print(c("acc:", acc))
# matriz de confusão
lvs <- c("1", "0")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
# load Caret package for computing Confusion matrix
library(caret)
confusionMatrix(xtab)
seqi<-seq(0,6, 0.1)
seqj<-seq(0,6, 0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-y_voted_percetron(x, w, c, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
#perceptron
y_voted_percetron <- function(xvec, w, c, pol){
# xvec: vetor de entrada
# w: vetor de pesos
# yp: resposta do Perceptron
s <- 0
for (i in 1:nrow(w)){
u <- xvec %*% w[i, 2:ncol(w)] + pol * w[i, 1]
s <- s + c[i] * sign(u)
}
return(sign(s))
}
# aplicação do perceptron para separação
train_voted_perceptron <- function(xin, yd, eta, tol, maxepocas, par){
# xin: matrin N x n com dados de entrada
# yd: rótulos de saída
# eta: passo do treinamento
# tol: tolerancia de erro
# maxepocas: numero maximo de iterações
# par: parametro de entrada
# par=0 ==> xin tem dimensão n+1 e já inclui entrada correspondente ao temo de polarização
# par =1 ==> xin tem dimensão n e não inclui entrada corresponde ao termo de polarização, que deve ser adicionado dentro da função
dimxin<-dim(xin)
N<-dimxin[1]
n<-dimxin[2]
if(par==1){
wt<-as.matrix(runif(n+1) - 0.5)
xin<-cbind(-1,xin)
}
else{
wt<-as.matrix(runif(n)-0.5)
}
nepocas<-0
eepoca<-tol+1
evec<-matrix(nrow=1,ncol=maxepocas)
wt <- matrix(0, ncol=ncol(xin), nrow=N)
c <- matrix(0, ncol=1, nrow=N)
while((nepocas<maxepocas && eepoca>tol)){
ei2<-0
xseq<-sample(N)
k <- 1
for(i in 1:N){
irand<-xseq[i]
yhati<-1.0*((xin[irand,] %*% wt[k,])>=0)
ei<-yd[irand]-yhati
if (ei == 0){ # acertou
c[k] <- c[k] + 1
} else { # errou
dw<-eta*ei*xin[irand,]
wt[k + 1,] <- wt[k,] + dw
c[k + 1,] <- 1
ei2<-ei2+ei*ei
k <- k + 1
}
}
nepocas<-nepocas+1
evec[nepocas]<-ei2/N
eepoca<-evec[nepocas]
}
retlist<-list(wt[1:k,], c[1:k,], evec[1:nepocas])
return(retlist)
}
eval_voted_perceptron <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do voted perceptron
sol <- train_voted_perceptron(x_train, y_train, 0.01, 0.01, 50, 1)
v <- sol[[1]]
c <- sol[[2]]
err <- c(0)
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- y_voted_percetron(x_test[i,], v, c, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("-1", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (c(cm$overall[1], err[length(err)]))
}
### FUNÇÕES PARA TESTAR O MÉTODO ###
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
# separação em treinamento e teste
suffled_indexes <- sample(nt)
train_size <- nt * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(train_size+1):nt], cbind(1,2)]
y_test <- X[suffled_indexes[(train_size+1):nt], 3]
# treinamento do perceptron
sol <- train_voted_perceptron(X_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
c <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(X_test)[1], ncol=1)
for (i in 1:dim(X_test)[1]){
s <- y_voted_percetron(X_test[i,], w, c,-1)
ypred[i] <- s
}
correct <- length(ypred[ypred==y_test])
total <- length(ypred)
acc <- correct/total
print(c("acc:", acc))
# matriz de confusão
lvs <- c("1", "0")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
# load Caret package for computing Confusion matrix
library(caret)
confusionMatrix(xtab)
seqi<-seq(0,6, 0.1)
seqj<-seq(0,6, 0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-y_voted_percetron(x, w, c, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
>>>>>>> Stashed changes
