time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
######################################### TWO GAUSSIAN DATASET #########################################
nclusters <- 5
nc = 500
xc1 <- matrix(0.45 * rnorm(nc) + 2.5, ncol = 2)
xc2 <- matrix(0.45 * rnorm(nc) + 3.5, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
km <- kmeans(as.matrix(X[,1:2]), nclusters, iter.max=50)
clusters <- as.matrix(km[[1]])
# plot cluster centers
points(km$centers, col="green", pch=16)
# plot samples
pchs <- c()
names <- c()
for (i in 1:nclusters){
pch = 5 + i
points(X[clusters == i,1], X[clusters == i,2], pch=pch)
pchs <- c(pchs, pch)
names <- c(names, sprintf("Cluster %s", i))
}
legend("topleft", legend=names, pch=pchs)
evaluate_dataset(X[,1:2], X[,3], name, nclusters, 100)
rm(list=ls())
graphics.off()
library(mlbench)
library(e1071)
library(caret)
library(kernlab)
library(GGClassification)
library(tsutils)
# MAIN
multiple_evaluations <- function(X, Y, num_clusters, num_eval, svm_kernel='rbfdot' ){
nclusters <- num_clusters
nm_acc <- c()
svm_acc <- c()
gg_acc <- c()
nm_time <- c()
svm_time <- c()
gg_time <- c()
# FUNÇÕES DE SUPORTE
get_cluster <- function(x, km){
centers <- km$centers
dists <- rowSums(x - centers)^2
cluster_id <- which(dists==min(dists))
return (cluster_id)
}
pred_func <- function (x, cluster_model, km, nclusters, svm_option, gg_option){
num_col <- ncol(x)
cluster_col <- num_col + 1
result_col <- num_col + 2
x <- cbind(x, 0)
# atribui clusters
for (i in 1:nrow(x)){
cluster_id <- get_cluster(x[i,], km)
x[i, cluster_col] <- cluster_id
}
x <- cbind(x, 0)
# print(nclusters)
for (cid in 1:nclusters){
if (is.null(nrow(x[x[, cluster_col]==cid,]))){
result_count <- 1
} else{
result_count <- nrow(x[x[, cluster_col]==cid,])
}
# print(sprintf("%s -> %s",cid, result_count))
if(result_count > 0){
model_name <- cluster_model[cid]
# print(sprintf("Cluster: %s - model: %s",cid, model_name))
if (model_name == "GG"){
input <- matrix(x[x[, cluster_col]==cid, 1:num_col], result_count, num_col)
x[x[, cluster_col]==cid, result_col] <- GGClassification::predict(gg_option, input)
}else if (model_name == "SVM"){
input <- matrix(x[x[, cluster_col]==cid, 1:num_col], result_count, num_col)
x[x[, cluster_col]==cid, result_col] <- kernlab::predict(svm_option, input, type='response')
}
}
# print("-")
}
return(x[, result_col])
}
eval_model <- function(x, y, model, n_exec, type){
n <- nrow(x)
eval_size <- floor(n * 0.99)
acc <- c()
for (i in 1:n_exec){
suffled_indexes <- sample(n)
x_ <- x[suffled_indexes[1:eval_size],]
y_ <- y[suffled_indexes[1:eval_size]]
if(type=="GG"){
pred <- GGClassification::predict(model, x_)
}else if(type=="SVM"){
pred <- kernlab::predict(model, x_)
}
acc <- c(acc, sum(pred == y_)/length(y_))
}
return (mean(acc))
}
for(neval in 1:num_eval){
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.80)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# clusterizar o dataset
nm_start_time <- Sys.time()
km <- kmeans(as.matrix(x_train), nclusters, iter.max=50)
clusters <- as.matrix(km[[1]])
# treinamento de svm em clusteres
svm_option <- kernlab::ksvm(x_train, y_train, type='C-svc', kernel=svm_kernel)
print("SVM_MODELO:")
svm_option
gg_option <- GGClassification::model(x_train, y_train)
print("GG_MODELO:")
gg_option
cluster_model <- c()
for (i in 1:nclusters){
# print(sprintf("treinando cluster %s", i))
x_ <-x_train[clusters == i,]
y_ <-y_train[clusters == i]
acc_svm <- eval_model(x_, y_, svm_option, n_exec=1, type="SVM")
# print(sprintf("SVM (acc): %s", acc_svm))
acc_gg <- eval_model(x_, y_, gg_option, n_exec=1, type="GG")
# print(sprintf("GG (acc): %s", acc_gg))
# print("--")
if (acc_gg > acc_svm){
cluster_model <- c(cluster_model, "GG")
}else{
cluster_model <- c(cluster_model, "SVM")
}
}
y_pred <- pred_func(x=x_test, cluster_model, km=km, nclusters=nclusters, svm_option, gg_option)
nm_end_time <- Sys.time()
# cm_nm <- confusionMatrix(as.factor(y_pred), as.factor(y_test))
# SVM
svm_start_time <- Sys.time()
svmtrein <- kernlab::ksvm(x_train, y_train, type='C-svc', kernel=svm_kernel)
predSVM <- kernlab::predict(svmtrein, x_test, type='response')
svm_end_time <- Sys.time()
# cm_svm <- confusionMatrix(as.factor(predSVM), as.factor(y_test))
#GG
gg_start_time <- Sys.time()
mdl <- GGClassification::model(x_train, y_train)
y_pred_gg <- predict(mdl, x_test)
gg_end_time <- Sys.time()
# cm_gg <- confusionMatrix(as.factor(y_pred_gg), as.factor(y_test))
nm_acc <- c(nm_acc, sum(y_pred == y_test)/length(y_test))
svm_acc <- c(svm_acc, sum(predSVM == y_test)/length(y_test))
gg_acc <- c(gg_acc, sum(y_pred_gg == y_test)/length(y_test))
nm_time <- c(nm_time, nm_end_time - nm_start_time)
svm_time <- c(svm_time, svm_end_time - svm_start_time)
gg_time <- c(gg_time, gg_end_time - gg_start_time)
print("EVALUATION")
print(sprintf("cluster model: %s (%s) ",sum(y_pred == y_test)/length(y_test),
nm_end_time - nm_start_time))
print(sprintf("SVM: %s ", sum(predSVM == y_test)/length(y_test),
svm_end_time - svm_start_time))
print(sprintf("ggClassification: %s ",sum(y_pred_gg == y_test)/length(y_test),
gg_end_time - gg_start_time))
print("----------------------------------------------------")
}
return(list(data.frame("nm" = nm_acc, "svm" = svm_acc, "gg" = gg_acc),
data.frame("nm" = nm_time, "svm" = svm_time, "gg" = gg_time)))
}
evaluate_dataset <- function(X, Y, name, nclusters, num_exec){
results <- multiple_evaluations(X, Y, nclusters, num_exec)
accs <- results[[1]]
times <- results[[2]]
par(mfrow=c(2,2))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(accs, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(times, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Tempo (%s)",name))
# Aplicação do kruskal e teste de nemenyi
accs_matrix <- as.matrix(accs)
tsutils::nemenyi(accs_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Acurácias)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
# Aplicação do kruskal e teste de nemenyi
time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
######################################### TWO GAUSSIAN DATASET #########################################
nclusters <- 5
nc = 500
xc1 <- matrix(0.45 * rnorm(nc) + 2.5, ncol = 2)
xc2 <- matrix(0.45 * rnorm(nc) + 3.5, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
km <- kmeans(as.matrix(X[,1:2]), nclusters, iter.max=50)
clusters <- as.matrix(km[[1]])
# plot cluster centers
points(km$centers, col="green", pch=16)
# plot samples
pchs <- c()
names <- c()
for (i in 1:nclusters){
pch = 5 + i
points(X[clusters == i,1], X[clusters == i,2], pch=pch)
pchs <- c(pchs, pch)
names <- c(names, sprintf("Cluster %s", i))
}
legend("topleft", legend=names, pch=pchs)
evaluate_dataset(X[,1:2], X[,3], name, nclusters, 100)
######################################### IRIS DATASET #########################################
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-setosa'] <- 'Iris-versicolor' #juntando classe 0 com classe 1
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
# plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 5]
evaluate_dataset(X, Y, name, 5, 100)
debugSource("~/Documentos/UFMGMasters/PatternRecognitionTechniques/ARTIGO3/main_script.R")
svm_option
View(svm_option)
svm_option$param
svm_option[1]
svm_option[[1]]
svm_option
svm_option.sigma
library(mlbench)
library(kernlab)
library(GGClassification)
library(caret)
library(spatgraphs)
# definição do spirals
N<-1000 #numeros pares
p <- mlbench.spirals(N,1,0.15)
x <- p[[1]]
x1 <- x[,1]
x2 <- x[,2]
y <- p[[2]]
spirals <- data.frame(x[,1], x[,2], y)
str(spirals)
# divisão em treinamento e teste
inTraining <- createDataPartition(spirals$y, p = .8, list = FALSE)
training <- spirals[ inTraining,]
testing  <- spirals[-inTraining,]
# SVM
svmtrein <- kernlab::ksvm(y ~ ., data = training, type='C-svc', kernel='rbfdot')
svmtrein
predSVM <- kernlab::predict(svmtrein, testing, type='response')
table(testing$y, predSVM)
kernlab::plot(svmtrein, data=spirals)
# GABRIEL GRAPH
a = x[y==1,]
b = x[y==2,]
g <- spatgraph(x, type="gabriel")
mdl <- GGClassification::model(x, y)
mdl
prd <- predict(mdl, x)
prd
# https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/points.html
plot(g, x)
points(a, col="blue", pch=1)
points(b, col="green", pch=1)
points(mdl$Midpoints, col="red", pch=19)
title(main="Grafo de Gabriel")
#dataset 2
nc = 500
xc1 <- matrix(0.45 * rnorm(nc) + 2.5, ncol = 2)
xc2 <- matrix(0.45 * rnorm(nc) + 3.5, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
suffled_indexes <- sample(nc)
train_size = nc * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(71:100)], cbind(1,2)]
y_test <- X[suffled_indexes[(71:100)], 3]
# SVM
svmtrein <- kernlab::ksvm(X[,1:2], X[,3], type='C-svc', kernel='rbfdot')
svmtrein
predSVM <- kernlab::predict(svmtrein, X_test, type='response')
table(y_test, predSVM)
kernlab::plot(svmtrein, data=X)
# LSSVM
lssvmtrein <- kernlab::ksvm(X[,1:2], X[,3], data = X_train, type='C-svc', kernel='rbfdot')
lssvmtrein
predLSSVM <- kernlab::predict(lssvmtrein, X_test, type='response')
table(y_test, predLSSVM)
kernlab::plot(lssvmtrein, data=X, main="t", xlab="oi")
a = X[X[,3] == 1,1:2]
b = X[X[,3] == 0,1:2]
g <- spatgraph(X[,1:2], type="gabriel")
mdl <- GGClassification::model(X_train, y_train)
mdl
prd <- predict(mdl, X_test)
prd
# https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/points.html
plot(g, X[,1:2])
points(a, col="blue", pch=1)
points(b, col="green", pch=1)
points(mdl$Midpoints, col="red", pch=19)
title(main="Grafo de Gabriel")
library(mlbench)
library(kernlab)
library(GGClassification)
library(caret)
library(spatgraphs)
# definição do spirals
N<-1000 #numeros pares
p <- mlbench.spirals(N,1,0.15)
x <- p[[1]]
x1 <- x[,1]
x2 <- x[,2]
y <- p[[2]]
spirals <- data.frame(x[,1], x[,2], y)
str(spirals)
# divisão em treinamento e teste
inTraining <- createDataPartition(spirals$y, p = .8, list = FALSE)
training <- spirals[ inTraining,]
testing  <- spirals[-inTraining,]
# SVM
svmtrein <- kernlab::ksvm(y ~ ., data = training, type='C-svc', kernel='rbfdot')
svmtrein
predSVM <- kernlab::predict(svmtrein, testing, type='response')
table(testing$y, predSVM)
kernlab::plot(svmtrein, data=spirals)
# GABRIEL GRAPH
a = x[y==1,]
b = x[y==2,]
g <- spatgraph(x, type="gabriel")
mdl <- GGClassification::model(x, y)
mdl
prd <- predict(mdl, x)
prd
# https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/points.html
plot(g, x)
points(a, col="blue", pch=1)
points(b, col="green", pch=1)
points(mdl$Midpoints, col="red", pch=19)
title(main="Grafo de Gabriel")
#dataset 2
nc = 500
xc1 <- matrix(0.45 * rnorm(nc) + 2.5, ncol = 2)
xc2 <- matrix(0.45 * rnorm(nc) + 3.5, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
suffled_indexes <- sample(nc)
train_size = nc * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(71:100)], cbind(1,2)]
y_test <- X[suffled_indexes[(71:100)], 3]
# SVM
svmtrein <- kernlab::ksvm(X[,1:2], X[,3], type='C-svc', kernel='rbfdot')
svmtrein
predSVM <- kernlab::predict(svmtrein, X_test, type='response')
table(y_test, predSVM)
kernlab::plot(svmtrein, data=X)
# LSSVM
lssvmtrein <- kernlab::ksvm(X[,1:2], X[,3], data = X_train, type='C-svc', kernel='rbfdot')
lssvmtrein
predLSSVM <- kernlab::predict(lssvmtrein, X_test, type='response')
table(y_test, predLSSVM)
kernlab::plot(lssvmtrein, data=X, main="t", xlab="oi")
a = X[X[,3] == 1,1:2]
b = X[X[,3] == 0,1:2]
g <- spatgraph(X[,1:2], type="gabriel")
mdl <- GGClassification::model(X_train, y_train)
mdl
prd <- predict(mdl, X_test)
prd
# https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/points.html
plot(g, X[,1:2])
points(a, col="blue", pch=1)
points(b, col="green", pch=1)
points(mdl$Midpoints, col="red", pch=19)
title(main="Grafo de Gabriel")
setwd("~/Documentos/UFMGMasters/NeuralNetworks/Artigo03")
rm(list=ls())
graphics.off()
source('perceptron.R')
source('votedPerceptron.R')
source('commiteePerceptron.R')
source('svm_model.R')
source('ggclass_model.R')
library(caret)
library(mlbench)
# MAIN
multiple_evaluations <- function(X, Y, num_eval){
acc1 <- matrix(0, nrow=num_eval, ncol=1)
acc2 <- matrix(0, nrow=num_eval, ncol=1)
acc3 <- matrix(0, nrow=num_eval, ncol=1)
acc4 <- matrix(0, nrow=num_eval, ncol=1)
acc5 <- matrix(0, nrow=num_eval, ncol=1)
t1 <- matrix(0, nrow=num_eval, ncol=1)
t2 <- matrix(0, nrow=num_eval, ncol=1)
t3 <- matrix(0, nrow=num_eval, ncol=1)
t4 <- matrix(0, nrow=num_eval, ncol=1)
t5 <- matrix(0, nrow=num_eval, ncol=1)
for (i in 1:num_eval){
print(sprintf("Iteration - %s", i))
ts1 <- Sys.time()
s1 <- eval_perceptron(X, Y, should_plot_matrix=FALSE)
te1 <- Sys.time()
ts2 <- Sys.time()
s2 <- eval_svm(X, Y, should_plot_matrix=FALSE)
te2 <- Sys.time()
ts3 <- Sys.time()
s3 <- eval_gg(X, Y, should_plot_matrix=FALSE)
te3 <- Sys.time()
ts4 <- Sys.time()
s4 <- eval_com_perceptron(X, Y, should_plot_matrix=FALSE)
te4 <- Sys.time()
ts5 <- Sys.time()
s5 <- eval_voted_perceptron(X, Y, should_plot_matrix=FALSE)
te5 <- Sys.time()
acc1[i] <- s1[1]
t1[i] <- te1 - ts1
acc2[i] <- s2[1]
t2[i] <- te2 - ts2
acc3[i] <- s3[1]
t3[i] <- te3 - ts3
acc4[i] <- s4[1]
t4[i] <- te4 - ts4
acc5[i] <- s5[1]
t5[i] <- te5 - ts5
# print(sprintf("perc: %s | com perc: %s | voted perc: %s \n\n", s1[1], s2[1], s3[1]))
}
return(list(data.frame("perceptron" = acc1, "svm" = acc2, "gg_classification" = acc3, "perceptron_commitee" = acc4, "voted_perceptron" = acc5),
data.frame("perceptron" = t1, "svm" = t2, "gg_classification" = t3, "perceptron_commitee" = t4, "voted_perceptron" = t5)))
}
evaluate_dataset <- function(X, Y, name, num_exec){
results <- multiple_evaluations(X, Y, num_exec)
accs <- results[[1]]
times <- results[[2]]
print(summary(accs))
print(sprintf("sd perceptron (%s)",sd(accs$perceptron)))
print(sprintf("sd svm (%s)",sd(accs$svm)))
print(sprintf("sd gg_classification (%s)",sd(accs$gg_classification)))
print(sprintf("sd perceptron_commitee (%s)",sd(accs$perceptron_commitee)))
print(sprintf("sd voted_perceptron (%s)",sd(accs$voted_perceptron)))
par(mfrow=c(2,2))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(accs, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
# Boxplot
par(mar=c(4,10,1,1))
boxplot(times, data=data,
las = 2,
horizontal = TRUE,
ann=FALSE
)
title(main = sprintf("Tempo (%s)",name))
# Aplicação do kruskal e teste de nemenyi
accs_matrix <- as.matrix(accs)
tsutils::nemenyi(accs_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Acurácias)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
# Aplicação do kruskal e teste de nemenyi
time_matrix <- as.matrix(times)
tsutils::nemenyi(time_matrix, conf.level=0.95,plottype="vmcb", sort=TRUE, main="Teste de Nemenyi (Tempo)") # possiveis plots: "vline", "none", "mcb", "vmcb", "line", "matrix"
}
######################################### TWO GAUSSIAN DATASET #########################################
nc = 500
xc1 <- matrix(0.35 * rnorm(nc) + 2, ncol = 2)
xc2 <- matrix(0.35 * rnorm(nc) + 4, ncol = 2)
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
plot(xc1[,1], xc1[,2], col="blue", xlim=c(1, 5), ylim=c(1, 5)) # plot do dataset
points(xc2[,1], xc2[,2], col="red", pch=1) # plot do dataset
title(main="Two Gaussians Dataset")
name <- "Two Gaussian"
evaluate_dataset(X[,1:2], X[,3], name, 50)
#########################################  SONAR DATASET #########################################
# load the dataset
name <- "Sonar Dataset"
sonar <- data(Sonar)
# converter em matrix
mat <- data.matrix(Sonar)
Y <- mat[, 61] - 1 # transformando entradas em 0 e 1
X <- scale(mat[, 0:60], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
evaluate_dataset(X, Y, name, 50)
