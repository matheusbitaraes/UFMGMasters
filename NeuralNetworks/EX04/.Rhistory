rm(list=ls())
library('plot3D')
source('trainperceptron.R')
source('yperceptron.R')
#### EX1
s1 <- 0.4
s2 <- 0.4
nc <- 100
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
plot(xc1[,1], xc1[,2], col='red', xlim=c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim=c(0,6), ylim=c(0,6), xlab='', ylab='')
title(main="Distribuições Exercicio 1")
x1_reta <- seq(6/100, 6, 6/100)
x2_reta <- -x1_reta+6
par(new=T)
plot(x1_reta, x2_reta, type='l', col='orange', xlim=c(0,6),ylim=c(0,6), xlab='', ylab='')
plot(x1_reta, x2_reta, type='l', col='orange', xlim=c(0,6),ylim=c(0,6), xlab='', ylab='')
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
sol <- trainperceptron(X[,1:2],X[,3], 0.01, 0.01, 10, 1)
w <- sol[[1]]
err <- sol[[2]]
seqi<-seq(0,6,0.1)
seqj<-seq(0,6,0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-yperceptron(x, w, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
rm(list=ls())
library('plot3D')
source('trainperceptron.R')
source('yperceptron.R')
#### EX1
s1 <- 0.4
s2 <- 0.4
nc <- 100
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
plot(xc1[,1], xc1[,2], col='red', xlim=c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim=c(0,6), ylim=c(0,6), xlab='', ylab='')
title(main="Distribuições Exercicio 1")
x1_reta <- seq(6/100, 6, 6/100)
x2_reta <- -x1_reta+6
par(new=T)
plot(x1_reta, x2_reta, type='l', col='orange', xlim=c(0,6),ylim=c(0,6), xlab='', ylab='')
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
sol <- trainperceptron(X[,1:2],X[,3], 0.01, 0.01, 10, 1)
w <- sol[[1]]
err <- sol[[2]]
seqi<-seq(0,6,0.1)
seqj<-seq(0,6,0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-yperceptron(x, w, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
rm(list=ls())
library('plot3D')
source('trainperceptron.R')
source('yperceptron.R')
#### EX1
s1 <- 0.4
s2 <- 0.4
nc <- 100
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
plot(xc1[,1], xc1[,2], col='red', xlim=c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim=c(0,6), ylim=c(0,6), xlab='', ylab='')
title(main="Distribuições Exercicio 1")
x1_reta <- seq(6/100, 6, 6/100)
x2_reta <- -x1_reta+6
par(new=T)
plot(x1_reta, x2_reta, type='l', col='orange', xlim=c(0,6),ylim=c(0,6), xlab='', ylab='')
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
sol <- trainperceptron(X[,1:2],X[,3], 0.01, 0.01, 10, 1)
w <- sol[[1]]
err <- sol[[2]]
seqi<-seq(0,6,0.1)
seqj<-seq(0,6,0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-yperceptron(x, w, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
rm(list=ls())
library('plot3D')
source('trainperceptron.R')
source('yperceptron.R')
#### EX1
s1 <- 0.4
s2 <- 0.4
nc <- 100
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
plot(xc1[,1], xc1[,2], col='red', xlim=c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim=c(0,6), ylim=c(0,6), xlab='', ylab='')
title(main="Distribuições Exercicio 1")
x1_reta <- seq(6/100, 6, 6/100)
x2_reta <- -x1_reta+6
par(new=T)
plot(x1_reta, x2_reta, type='l', col='orange', xlim=c(0,6),ylim=c(0,6), xlab='', ylab='')
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
sol <- trainperceptron(X[,1:2],X[,3], 0.01, 0.01, 10, 1)
w <- sol[[1]]
err <- sol[[2]]
seqi<-seq(0,6,0.1)
seqj<-seq(0,6,0.1)
M<-matrix(0,nrow=length(seqi), ncol=length(seqj))
ci<-0
for(i in seqi){
ci<-ci+1
cj<-0
for(j in seqj){
cj<-cj+1
x<-c(i,j)
M[ci,cj]<-yperceptron(x, w, -1)
}
}
plot(xc1[,1], xc1[,2], col='red', xlim = c(0,6), ylim=c(0,6), xlab='x_1', ylab='x_2')
par(new=T)
plot(xc2[,1], xc2[,2], col='blue', xlim = c(0,6), ylim=c(0,6), xlab='', ylab='')
par(new=T)
contour(seqi, seqj, M, xlim=c(0,6), ylim=c(0,6))
title("Região de Separação (2D)")
persp3D(seqi,seqj,M,counter=T, theta=55, phi=30, r=40, d=0.1, expand=0.5,
ltheta=90, lphi=180, shade=0.4, ticktype="detailed", nticks=5)
title("Região de Separação (3D)")
# gerar mais amostras da mesma distribuição
# criação de dataset com duas distribuições normais
s1 <- 0.4
s2 <- 0.4
nc <- 200
nt <- nc * 2
# criação de dataset com duas distribuições normais
xc1 <- matrix(rnorm(nc * 2), ncol = 2)*s1 + t(matrix((c(2,2)),ncol=nc,nrow=2))
xc2 <- matrix(rnorm(nc * 2), ncol = 2)*s2 + t(matrix((c(4,4)),ncol=nc,nrow=2))
xc1 <- cbind(xc1, rep(0, times = nc/2))
xc2 <- cbind(xc2, rep(1, times = nc/2))
X <- rbind(xc1, xc2)
# separação em treinamento e teste
suffled_indexes <- sample(nt)
train_size <- nt * 0.7
X_train <- X[suffled_indexes[1:train_size], cbind(1,2)]
y_train <- X[suffled_indexes[1:train_size], 3]
X_test <- X[suffled_indexes[(train_size+1):nt], cbind(1,2)]
y_test <- X[suffled_indexes[(train_size+1):nt], 3]
# treinamento do perceptron
sol <- trainperceptron(X_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(X_test)[1], ncol=1)
for (i in 1:dim(X_test)[1]){
s <- yperceptron(X_test[i,], w, -1)
ypred[i] <- s
}
correct <- length(ypred[ypred==y_test])
total <- length(ypred)
acc <- correct/total
print(c("acc:", acc))
# matriz de confusão
lvs <- c("1", "0")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
# load Caret package for computing Confusion matrix
library(caret)
confusionMatrix(xtab)
plot(1:length(acc), acc)
plot(1:length(accs), accs)
# carrega iris dataset
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 2 com classe 3
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # normalização de dados
Y <- mat[, 5] - 1
eval <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do perceptron
sol <- trainperceptron(x_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- yperceptron(x_test[i,], w, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("0", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (cm$overall[1])
}
acc <- eval(X, Y, should_plot_matrix=TRUE)
acc
# repete várias vezes o procedimento
n <- 100
accs <- matrix(0, nrow=n, ncol=1)
for (i in 1:n){
accs[i] <- eval(X, Y, should_plot_matrix=FALSE)
}
plot(1:length(accs), accs)
boxplot(accs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
summary(accs)
# carrega iris dataset
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 2 com classe 3
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # normalização de dados
Y <- mat[, 5] - 1
eval <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do perceptron
sol <- trainperceptron(x_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- yperceptron(x_test[i,], w, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("0", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (err[length(err)])
}
err <- eval(X, Y, should_plot_matrix=TRUE)
err
# repete várias vezes o procedimento
n <- 100
errs <- matrix(0, nrow=n, ncol=1)
for (i in 1:n){
errs[i] <- eval(X, Y, should_plot_matrix=FALSE)
}
plot(1:length(errs), errs)
boxplot(errs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = sprintf("Distribuição Erro percentual",name))
summary(errs)
eval <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do perceptron
sol <- trainperceptron(x_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- yperceptron(x_test[i,], w, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("0", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (c(cm$overall[1], err[length(err)]))
}
s <- eval(X, Y, should_plot_matrix=TRUE)
acc <- s[1]
err <- s[2]
# repete várias vezes o procedimento
n <- 100
errs <- matrix(0, nrow=n, ncol=1)
for (i in 1:n){
s <- eval(X, Y, should_plot_matrix=FALSE)
accs[i] <- s[1]
errs[i] <- s[2]
}
plot(1:length(errs), errs)
boxplot(errs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = sprintf("Distribuição Erro percentual",name))
summary(errs)
boxplot(accs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = sprintf("Acurácia (%)",name))
summary(accs)
boxplot(accs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = sprintf("Acurácia (%)",name))
title(main ="Distribuição Erro percentual")
# carrega iris dataset
name <- "Iris Dataset"
iris.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <- read.csv(iris.url, header=FALSE)
# pre processamento
iris$V5[iris$V5 == 'Iris-virginica'] <- 'Iris-versicolor' #juntando classe 2 com classe 3
iris$V5 <- factor(iris$V5)
iris$V5 <- as.factor(iris$V5)
names(iris)[5] <- 'y'
plot(iris)
# converter em matrix
mat <- data.matrix(iris)
X <- scale(mat[, 1:4], center = TRUE, scale = TRUE) # normalização de dados
Y <- mat[, 5] - 1
eval <- function(X, Y, should_plot_matrix=FALSE){
# separação em treinamento e teste
nc <- nrow(X)
suffled_indexes <- sample(nc)
train_size <- floor(nc * 0.70)
x_train <- X[suffled_indexes[1:train_size],]
y_train <- Y[suffled_indexes[1:train_size]]
x_test <- X[suffled_indexes[(train_size+1):nc],]
y_test <- Y[suffled_indexes[(train_size+1):nc]]
# treinamento do perceptron
sol <- trainperceptron(x_train, y_train, 0.01, 0.01, 50, 1)
w <- sol[[1]]
err <- sol[[2]]
# acurácia
ypred <- matrix(0,nrow=dim(x_test)[1], ncol=1)
for (i in 1:dim(x_test)[1]){
s <- yperceptron(x_test[i,], w, -1)
ypred[i] <- s
}
# matriz de confusão
lvs <- c("0", "1")
truth <-  factor(y_test, levels = rev(lvs))
pred <- factor(ypred, levels = rev(lvs))
xtab <- table(pred, truth)
cm <- confusionMatrix(xtab)
if (should_plot_matrix){
print(cm)
}
return (c(cm$overall[1], err[length(err)]))
}
s <- eval(X, Y, should_plot_matrix=TRUE)
acc <- s[1]
err <- s[2]
# repete várias vezes o procedimento
n <- 100
errs <- matrix(0, nrow=n, ncol=1)
for (i in 1:n){
s <- eval(X, Y, should_plot_matrix=FALSE)
accs[i] <- s[1]
errs[i] <- s[2]
}
plot(1:length(errs), errs)
boxplot(errs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main ="Distribuição Erro percentual")
summary(errs)
boxplot(accs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = "Acurácia (%)")
summary(accs)
# carrega base de dados breast cancer
name <- "Breast Cancer Dataset"
bc.url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
bc <- read.csv(bc.url, header=FALSE)
bc <- bc[,2:11]
bc <- na.omit(bc) #limpeza de dados
bc <- droplevels(bc[!bc$V7 == '?',])
# pre processamento
bc$V11 <- as.factor(bc$V11)
names(bc)[10] <- "y"
# converter em matrix
mat <- data.matrix(bc)
X <- scale(mat[, 1:9], center = TRUE, scale = TRUE) # fase de normalização dos dados para o gg classification
Y <- mat[, 10] - 1
# repete várias vezes o procedimento
n <- 30
accs <- matrix(0, nrow=n, ncol=1)
for (i in 1:n){
accs[i] <- eval(X, Y, should_plot_matrix=FALSE)
}
boxplot(accs, data=data,
las = 2,
horizontal = FALSE,
ann=FALSE
)
title(main = sprintf("Acurácias (%s)",name))
summary(accs)
